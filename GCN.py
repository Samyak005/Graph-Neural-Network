# -*- coding: utf-8 -*-
"""Q2_GCN_scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16xU1aXLo8BXvgIgSMspYYlxlVDBFHT-k
"""
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

import torchsummary
import random

import torch_geometric
import torch_geometric.transforms as T

from torch_geometric.data import Data
from torch_geometric.datasets import Planetoid

torch.manual_seed(0)
random.seed(0)
np.random.seed(0)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("device:", device)

dataset = Planetoid(root="CiteSeer", name= "CiteSeer")

print("number of graphs:", len(dataset))
print("number of classes:", dataset.num_classes)
print("number of features:", dataset.num_features)
print("number of node features:", dataset.num_node_features)
print("number of edge features:", dataset.num_edge_features)

print(dataset.data)

import networkx as nx
from torch_geometric.utils import to_networkx

adj = nx.adjacency_matrix(to_networkx(dataset.data))

# A_hat = torch.as_tensor(adj.todense()) + torch.eye(adj.shape[0])

# print(torch.as_tensor(adj.todense()))

# print(adj)

# print(torch.eye(adj.shape[0]))

# D     = torch.diag(torch.sum(torch.Tensor(adj.todense()),1))

# print(torch.Tensor(np.sqrt(np.linalg.pinv(D))))

# print(torch.Tensor(np.linalg.pinv(D)).sqrt())

class GCNConv(nn.Module):
    def __init__(self, A, in_channels, out_channels):
        # super(GCNConv, self).__init__()
        super().__init__()
        A = torch.Tensor(A)
        self.A_hat = A + torch.eye(A.shape[0])
        self.D     = torch.diag(torch.sum(A,1))
        self.D = torch.Tensor(np.linalg.pinv(self.D)).sqrt()
        # self.D     = self.D.inverse().sqrt()
        self.A_hat = torch.mm(torch.mm(self.D, self.A_hat), self.D)
        self.W     = nn.Parameter(torch.rand(in_channels,out_channels))
    
    def forward(self, X):
        out = torch.relu(torch.mm(torch.mm(self.A_hat, X), self.W))
        return out

class CiteSeerNet(torch.nn.Module):
    def __init__(self, adj):
        # super(CiteSeerNet, self).__init__()
        super().__init__()
        self.conv1 = GCNConv(adj, dataset.num_features, dataset.num_classes)

        # Add a SAGEConv layer with 'max' as the aggregation function
        # self.conv2 = 

        # Add few more layers if you wish (more convs, dropout, etc) but you should output 'dataset.num_classes' logits at the end

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.conv1(x)

        # relu -> conv2 -> ...
        x = F.relu(x)

        return F.log_softmax(x, dim=1)

def train(model, data, optimizer, criterion):
    model.train()
    # 1. zero existing gradients
    optimizer.zero_grad()
    # 2. forward the graph
    output = model(data)
    # 3. compute the loss (use output[data.train_mask] and data.y[data.train_mask] to compute loss on training nodes only)
    loss = F.nll_loss(output[data.train_mask], data.y[data.train_mask])
    # 4. backprop the loss
    loss.backward()
    # 5. update the weights
    optimizer.step()
    return loss.item()

def test(model, data, mask, criterion):
    model.eval()
    # 1. forward the graph
    output = model(data)
    # 2. find labels for the nodes in the mask (output[mask]?)
    _, predicted_class = output.max(dim=1)
    predicted_class = predicted_class[mask]
    # print(predicted_class.shape)
    # print(data.y[mask].shape)
    return (predicted_class == data.y[mask]).sum()/ mask.sum()

model = CiteSeerNet(adj.todense()).to(device=device)
data = dataset.data.to(device=device)

criterion = nn.NLLLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

best_val_accuracy = 0
for epoch in range(600):
  train_loss = train(model, data, optimizer, criterion)  
  val_accuracy = test(model, data, data.val_mask, criterion)

  if val_accuracy > best_val_accuracy:
    print("new best model")
    torch.save(model.state_dict(), "best_model.pth")
    best_val_accuracy = val_accuracy

    print("[%d] training loss: %.3f" % (epoch + 1, train_loss))
    print("[%d] validation >> accuracy: %.2f%%" % (epoch + 1, val_accuracy * 100))
    print()

print("After No adjacency matrix normalization and", epoch+1, "epochs -", "Training Loss:", train_loss, 
        "Val accuracy:", float(val_accuracy*100))

##############################################################################################
"""# Symmetric Adjacency Matrix Normalization"""

import scipy.sparse as sp

def normalize_adj(adj):
    """Symmetrically normalize adjacency matrix."""
    adj = sp.coo_matrix(adj)
    rowsum = np.array(adj.sum(1))
    d_inv_sqrt = np.power(rowsum, -0.5).flatten()
    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.
    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)
    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)

adj_sym_normalized = normalize_adj(adj)

model = CiteSeerNet(adj_sym_normalized.todense()).to(device=device)
data = dataset.data.to(device=device)

criterion = nn.NLLLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

best_val_accuracy = 0
for epoch in range(800):
  train_loss = train(model, data, optimizer, criterion)  
  val_accuracy = test(model, data, data.val_mask, criterion)

  if val_accuracy > best_val_accuracy:
    print("new best model")
    torch.save(model.state_dict(), "best_model.pth")
    best_val_accuracy = val_accuracy

    print("[%d] training loss: %.3f" % (epoch + 1, train_loss))
    print("[%d] validation >> accuracy: %.2f%%" % (epoch + 1, val_accuracy * 100))
    print()

print("After Symmetric adjacency matrix normalization and", epoch+1, "epochs -", "Training Loss:", train_loss, 
        "Val accuracy:", float(val_accuracy*100))
##############################################################################################

"""# Row Adjacency Matrix Normalization"""

from sklearn.preprocessing import normalize

matrix = adj.todense()
row_normed_matrix = normalize(matrix, axis=1, norm='l1')
# print(row_normed_matrix)

model = CiteSeerNet(row_normed_matrix).to(device=device)
data = dataset.data.to(device=device)

criterion = nn.NLLLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

best_val_accuracy = 0
for epoch in range(600):
  train_loss = train(model, data, optimizer, criterion)  
  val_accuracy = test(model, data, data.val_mask, criterion)

  if val_accuracy > best_val_accuracy:
    print("new best model")
    torch.save(model.state_dict(), "best_model.pth")
    best_val_accuracy = val_accuracy

    print("[%d] training loss: %.3f" % (epoch + 1, train_loss))
    print("[%d] validation >> accuracy: %.2f%%" % (epoch + 1, val_accuracy * 100))
    print()

print("After Row adjacency matrix normalization and", epoch+1, "epochs -", "Training Loss:", train_loss, 
        "Val accuracy:", float(val_accuracy*100))
##############################################################################################

"""# Column Adjacency Matrix Normalization"""

column_normed_matrix = normalize(matrix, axis=0, norm='l1')
# print(column_normed_matrix)

model = CiteSeerNet(column_normed_matrix).to(device=device)
data = dataset.data.to(device=device)

criterion = nn.NLLLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

best_val_accuracy = 0
for epoch in range(600):
  train_loss = train(model, data, optimizer, criterion)  
  val_accuracy = test(model, data, data.val_mask, criterion)

  if val_accuracy > best_val_accuracy:
    print("new best model")
    torch.save(model.state_dict(), "best_model.pth")
    best_val_accuracy = val_accuracy

    print("[%d] training loss: %.3f" % (epoch + 1, train_loss))
    print("[%d] validation >> accuracy: %.2f%%" % (epoch + 1, val_accuracy * 100))
    print()

print("After column adjacency matrix normalization and", epoch+1, "epochs -", "Training Loss:", train_loss, 
        "Val accuracy:", float(val_accuracy*100))